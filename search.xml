<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>UniTST: Effectively Modeling Inter-Series and Intra-Series Dependencies for Multivariate Time Series Forecasting</title>
      <link href="/paper%20notes/UniTST/"/>
      <url>/paper%20notes/UniTST/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference:<br>authors: Juncheng Liu, Chenghao Liu, Gerald Woo, Yiwei Wang, Bryan Hooi, Caiming Xiong, Doyen Sahoo<br>code:</p></div>  <p>#Transformer</p><p><img src="/img/UniTST-1.png" alt=""></p><h2 id="Flatten-patches">Flatten patches</h2><p>Like Moirai [^1]</p><h3 id="Dispatcher-mechanism">Dispatcher mechanism</h3><p>add k(k&lt;&lt;N) learnable embeddings as dispatchers and use cross attention to distribute the dependencies among tokens. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(kN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span></p><p>[^1]: Unified Training of Universal Time Series Forecasting Transformers</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> æ—¶é—´åºåˆ— </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress</title>
      <link href="/paper%20notes/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/paper%20notes/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICDE 2022<br>authors: Reie Wu, Eam onn J. Keogh<br>code:</p></div><p>æå‡ºäº† UCR å¼‚å¸¸æ£€ æµ‹æ•°æ®é›†ï¼Œåˆ†æç°æœ‰å‡ ä¸ªæ•°æ®é›†ï¼ˆYahoo, Numenta, NASA, OMNI/SMDï¼‰çš„å››ä¸ªç¼ºé™·ã€‚</p><h2 id="1-Triviality">1. Triviality</h2><p>å®¹æ˜“è§£å†³çš„ï¼Œå¼‚å¸¸å¯ä»¥é€šè¿‡â€œä¸€è¡Œä»£ç â€ï¼ˆmean, max, std, diffâ€¦ï¼‰è½»æ˜“å‘ç°ã€‚<br><img src="/img/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86-1.png" alt=""></p><h2 id="2-Unrealistic-Anomaly-Density">2. Unrealistic Anomaly Density</h2><p>ä¸åˆ‡å®é™…çš„å¼‚å¸¸å¯†åº¦ï¼ŒNASA D-2, M-1 å’Œ M-2ï¼Œä¸€åŠä»¥ä¸Šæµ‹è¯•æ•°æ®è¢«æ ‡ä¸ºå¼‚å¸¸ã€‚è¿™ç§ä¸åˆ‡å®é™…çš„å¼‚å¸¸å¯†åº¦å­˜åœ¨è®¸å¤šé—®é¢˜ã€‚é¦–å…ˆï¼Œ<strong>å®ƒä¼¼ä¹æ¨¡ç³Šäº†åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ä¹‹é—´çš„ç•Œé™</strong>ã€‚åœ¨å¤§å¤šæ•°ç°å®ç¯å¢ƒä¸­ï¼Œé¢„è®¡å¼‚å¸¸çš„å…ˆéªŒæ¦‚ç‡ä»…ç•¥å¤§äºé›¶ã€‚è®©ä¸€åŠçš„æ•°æ®ç”±å¼‚å¸¸ç»„æˆä¼¼ä¹è¿åäº†è¿™é¡¹ä»»åŠ¡çš„æœ€åŸºæœ¬å‡è®¾ã€‚æ­¤å¤–ï¼Œè®¸å¤šç®—æ³•å¯¹å…ˆéªŒéå¸¸æ•æ„Ÿã€‚</p><h2 id="3-Mislabeled-Ground-Truth">3. Mislabeled Ground Truth</h2><p>æ ‡ç­¾é”™è¯¯</p><blockquote><p>C æ‰€æŒ‡æ®µå’Œ D æ®µå‡ ä¹ä¸€è‡´ï¼ŒD ä¸ºå¼‚å¸¸ç‚¹è€Œ C æ˜¯æ­£å¸¸ç‚¹<br><img src="/img/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86-2.png" alt=""></p></blockquote><blockquote><p>F å’Œ E éƒ½ä¸ºå¼‚å¸¸<br><img src="/img/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86-3.png" alt=""></p></blockquote><h2 id="4-Run-to-faiure-Bias">4. Run-to-faiure Bias</h2><p>Yahoo å’Œ NASAï¼Œå¼‚å¸¸å‡ºç°åœ¨æµ‹è¯•é›†çš„æœ«å°¾ (Many realworld systems are run-to-failure)<br><img src="/img/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86-4.png" alt=""></p><h2 id="5-å»ºè®®">5. å»ºè®®</h2><ul><li><p>å¼ƒç”¨ç›¸å…³æ•°æ®é›†</p></li><li><p><strong>åº”å‚è€ƒç®—æ³•çš„ä¸å˜æ€§æ¥è§£é‡Šç®—æ³•</strong><br>ä¸å˜æ€§åŒ…æ‹¬ï¼šå¹…åº¦ç¼©æ”¾ï¼Œåç§»ï¼Œé®æŒ¡ï¼Œå™ªå£°ï¼Œçº¿æ€§è¶‹åŠ¿ï¼Œæ‰­æ›²ï¼Œå‡åŒ€ç¼©æ”¾ç­‰ç­‰ã€‚<br><img src="/img/Benchmark_UCR%E6%95%B0%E6%8D%AE%E9%9B%86-5.png" alt=""></p><blockquote><p>åœ¨æ•°æ®ä¸­æ·»åŠ å™ªéŸ³ï¼ŒDiscord ä»ç„¶å¯ä»¥æ­£ç¡®æ£€æµ‹é™¤å¼‚å¸¸ï¼Œè¿™ä¾‹å­è¡¨æ˜ï¼Œå¦‚æœé‡åˆ°å™ªå£°æ•°æ®ï¼ŒDiscord è¡¨ç°çš„ä¼šæ¯” Telemanom å¥½ã€‚</p></blockquote></li><li><p><strong>å¯è§†åŒ–æ•°æ®å’Œç®—æ³•è¾“å‡º</strong><br>â€œæ—¶é—´åºåˆ—åˆ†ææœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè§†è§‰é¢†åŸŸâ€ã€‚</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¼‚å¸¸æ£€æµ‹ </tag>
            
            <tag> ICDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Drift doesnâ€™t Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection</title>
      <link href="/paper%20notes/D3R/"/>
      <url>/paper%20notes/D3R/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #NeurIPS 2023<br>authors: Chesen Wang, Zir ui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, Haifeng Sun, Jianxin Liao<br>code:</p></div><p><img src="/img/D3R-1.png" alt=""><br>Diffusion ç“¶é¢ˆ (å™ªéŸ³æ˜¯ç“¶é¢ˆï¼Œæœªæ±¡æŸ“æ•°æ®æ˜¯æ¡ä»¶)ï¼Œç“¶é¢ˆå±‚ä¸å†æ˜¯æ¨¡å‹çš„å±æ€§ï¼Œæ‰€ä»¥å¯ä»¥åŠ¨æ€è®¾ç½®æ— éœ€å†é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeurIPS </tag>
            
            <tag> å¼‚å¸¸æ£€æµ‹ </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MICN: MULTI-SCALE LOCAL AND GLOBAL CONTEXT MODELING FOR LONG-TERM SERIES FORECASTING</title>
      <link href="/paper%20notes/MICN/"/>
      <url>/paper%20notes/MICN/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICLR 2023<br>authors: Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, Yifei Xiao<br>code:</p></div><h2 id="0-Overview">0. Overview</h2><p><img src="/img/MICN-1.png" alt=""><br>å…ˆæ˜¯å°†è¾“å…¥åºåˆ—é€åˆ°<strong>å¤šå°ºåº¦æ··åˆåˆ†è§£æ¨¡å—</strong>ä¸­è¿›è¡Œåºåˆ—åˆ†è§£ï¼Œå¾—åˆ° Seasonal é¡¹å’Œ Trend-Cyclical é¡¹ï¼Œåˆ†åˆ«å¯¹ä¸¤è€…ç‹¬ç«‹è¿›è¡Œé¢„æµ‹ï¼Œæœ€åå°†é¢„æµ‹ç»“æœåŠ èµ·æ¥ã€‚å¯¹äº Trend-Cyclical é¡¹ï¼Œç›´æ¥é‡‡ç”¨çº¿æ€§å›å½’çš„æ–¹å¼ï¼Œå³ Trend-Cyclical Prediction Block å°±æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼›å¯¹äº Seasonal é¡¹ï¼Œé‡‡ç”¨æå‡ºçš„ MIC å±‚è¿›è¡Œé¢„æµ‹ã€‚</p><h2 id="1-å¤šå°ºåº¦">1. å¤šå°ºåº¦</h2><p>ç”¨å¹³å‡æ± åŒ–å¾—åˆ° Trend-Cyclical é¡¹ï¼Œç„¶ååŸå§‹åºåˆ—å‡å» Trend-Cyclical é¡¹å°±å¾—åˆ°äº† Seasonal é¡¹ã€‚è€ƒè™‘åˆ°<strong>å¹³å‡æ± åŒ–çš„ kernel å¤§å°æ§åˆ¶ç€åˆ†è§£çš„ä¸åŒæ¨¡å¼</strong>ï¼Œå› æ­¤ä½œè€…ç»¼åˆå¤šä¸ª kernel çš„å¹³å‡æ± åŒ–ç»“æœï¼Œå°†è¿™äº›ç»“æœå†å–ä¸€ä¸ªå¹³å‡ï¼Œå¾—åˆ° Trend-Cyclical é¡¹ã€‚</p><p><a href="https://zhuanlan.zhihu.com/p/603468264">(2023 ICLR) MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting - çŸ¥ä¹ (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting</title>
      <link href="/paper%20notes/PatchMixer/"/>
      <url>/paper%20notes/PatchMixer/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference:<br>authors: Zeying Gong, Yujin Tang, Junwei Liang<br>code:</p></div><p><img src="/img/PatchMixer-1.png" alt=""></p><h2 id="1-Periodical-Patch-and-Sliding-Window-Patch">1. Periodical Patch and Sliding Window Patch</h2><p><strong>Top-k Frequencies</strong>ï¼šè¿™ç§æ–¹æ³•é¦–å…ˆå°†åŸå§‹çš„æ—¶é—´åºåˆ—æ˜ å°„åˆ°é¢‘åŸŸï¼Œç„¶åé€‰æ‹©é¢‘åŸŸä¸»è¦æˆåˆ†ï¼Œå°†æ¯ä¸ªä¸»æˆåˆ†çš„åºåˆ—ç»„ç»‡æˆä¸€ä¸ªäºŒç»´ç»“æ„ã€‚è¿™é‡Œæ¯ä¸ªæˆåˆ†çš„æ—¶é—´åºåˆ—å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ª patchã€‚<br><strong>Sliding Window</strong>ï¼šè¿™ç§æ–¹å¼æ ¹æ®ä¸€ä¸ªçª—å£é•¿åº¦å’Œæ­¥é•¿åœ¨åŸå§‹çš„æ—¶é—´åºåˆ—ä¸Šç§»åŠ¨ï¼ŒæŠ½å–å‡ºå¤šä¸ªå­åºåˆ—ï¼Œè¿™äº›å­åºåˆ—ç»„ç»‡æˆä¸€ä¸ª patchã€‚è¿™é‡Œæ¯ä¸ªå­åºåˆ—å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ª patchã€‚<br>ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯åœ¨é¢‘åŸŸä¸Šé€šè¿‡é¢‘ç‡æˆåˆ†è¿›è¡Œ patch åˆ†å‰²ï¼Œå¦ä¸€ç§æ˜¯åœ¨æ—¶åŸŸä¸Šå°†åºåˆ—æŒ‰ç…§æ—¶é—´åˆ†æˆå¤šä¸ªç‰‡æ®µå¾—åˆ° patchã€‚<br>åœ¨å¾—åˆ°æ¯ä¸ª patch çš„æ•°æ®åï¼Œä¸‹ä¸€æ­¥æ˜¯ç”Ÿæˆæ¯ä¸ª patch çš„ embeddingã€‚åœ¨ä¹‹å‰çš„ Transformer ç±»å‹çš„å·¥ä½œä¸­ï¼Œä¸€èˆ¬ä¼šå¼•å…¥ position embedding è§£å†³ Transformer æ— æ³•å»ºæ¨¡æ—¶åºçš„é—®é¢˜ã€‚è€Œæœ¬æ–‡ä¸­ï¼Œç”±äºé‡‡ç”¨äº† CNN çš„ç»“æ„ï¼Œ<strong>å¤©ç„¶å…·å¤‡å¯¹åºåˆ—çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå› æ­¤æ–‡ä¸­æ²¡æœ‰å¼•å…¥ä»»ä½• position embedding</strong>ï¼Œè€Œæ˜¯ç›´æ¥é€šè¿‡ä¸€ä¸ª MLP å°† patch å†…çš„åºåˆ—æ•°æ®æ˜ å°„æˆ embeddingã€‚</p><h2 id="2-Local-and-global-info">2. Local and global info</h2><p><img src="/img/PatchMixer-2.png" alt=""><br>ä¸¤ä¸ªå·ç§¯åˆ†æ”¯åˆ†åˆ«æå–åºåˆ—çš„å±€éƒ¨ä¿¡æ¯å’Œå…¨å±€ä¿¡æ¯ã€‚å¯¹äºå±€éƒ¨ä¿¡æ¯åˆ†æ”¯ï¼Œä½¿ç”¨ä¸€ä¸ªå·ç§¯åœ¨æ¯ä¸ª patch å†…è¿›è¡Œ depthwise çš„å·ç§¯ï¼ˆpatch-inï¼‰ï¼Œå®ç° patch ç»´åº¦çš„å±€éƒ¨å»ºæ¨¡ã€‚åŒæ—¶ï¼Œåœ¨å…¨å±€ä¿¡æ¯åˆ†æ”¯ï¼Œä½¿ç”¨ä¸€ä¸ªè·¨ patch çš„å·ç§¯ï¼ˆpatch-wiseï¼‰ï¼Œå»ºæ¨¡ patch ä¹‹é—´çš„å…³ç³»ã€‚</p><h2 id="3-Dual-Head">3. <code>Dual Head</code></h2><p>==é€šè¿‡è·¨è¶Šå·ç§¯çš„çº¿æ€§æ®‹å·®è¿æ¥æ¥æå–æ—¶é—´å˜åŒ–çš„æ€»ä½“è¶‹åŠ¿ï¼ˆè¶‹åŠ¿é¡¹ï¼Œç§»åŠ¨å¹³å‡ï¼‰ï¼Œå¹¶åœ¨å¸¦æœ‰éçº¿æ€§å‡½æ•°çš„å…¨å·ç§¯å±‚ä¹‹åä½¿ç”¨ MLP é¢„æµ‹å¤´æ¥ç»†è‡´åœ°æ‹Ÿåˆé¢„æµ‹æ›²çº¿ä¸­çš„å¾®å°å˜åŒ–ã€‚==å„è‡ªçš„è¾“å‡ºç›¸åŠ æ¥å¾—å‡ºé¢„æµ‹ç»“æœã€‚ä¸ç›´æ¥ä½¿ç”¨å•ä¸ªçº¿æ€§ flatten å¤´ç›¸æ¯”ï¼Œä½¿ç”¨åŒå¤´å¯ä»¥äº§ç”Ÿæ›´æœ‰æ•ˆçš„æ˜ å°„æ•ˆæœã€‚</p><h2 id="4-æ·±åº¦å¯åˆ†ç¦»å·ç§¯">4. æ·±åº¦å¯åˆ†ç¦»å·ç§¯</h2><p><a href="https://zhuanlan.zhihu.com/p/155584110">æ·±å…¥æµ…å‡ºå¯åˆ†ç¦»å·ç§¯ - çŸ¥ä¹ (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TIMESNET:TEMPORAL 2D-VARIATION MODELING FOR GENERAL TIME SERIES ANALYSIS</title>
      <link href="/paper%20notes/TimesNet/"/>
      <url>/paper%20notes/TimesNet/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICLR 2023<br>authors: Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, Mingsheng Long<br>code: <a href="https://github.com/thuml/TimesNet">https://github.com/thuml/TimesNet</a></p></div><p><img src="/img/TimesNet.png" alt=""></p><p><img src="/img/TimesNet-1.png" alt=""></p><p><img src="/img/TimesNet-2.png" alt=""></p><p>é€šè¿‡ FFT å‘ç°å‘¨æœŸï¼ŒAmp() è¡¨ç¤ºå¹…å€¼ã€‚Avg æ˜¯åœ¨ c ä¸ªç‰¹å¾é€šé“ä¸Šåšå¹³å‡ã€‚æŒ‘é€‰å‡º k ä¸ªå¹…å€¼æœ€å¤§çš„é¢‘ç‡ï¼Œå‘¨æœŸé•¿åº¦ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><mi>T</mi><msub><mi>f</mi><mi>i</mi></msub></mfrac><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>âˆ—</mo><msub><mi>f</mi><mi>i</mi></msub><mo>=</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_i=\frac{T}{f_i}, (p_i*f_i=T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3534em;vertical-align:-0.4811em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.1076em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span> ã€‚</p><p>reshape æˆ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>âˆ—</mo><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i*f_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>ï¼Œè½¬ 2D Spaceï¼Œç»ç›¸å…³ 2D vision backbone åï¼Œå† reshape backã€‚</p><p><img src="/img/TimesNet-3.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift</title>
      <link href="/paper%20notes/RevIN/"/>
      <url>/paper%20notes/RevIN/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICLR 2022<br>authors: Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, Jaegul Choo<br>code: <a href="https://github.com/ts-kim/RevIN">https://github.com/ts-kim/RevIN</a></p></div><p><img src="/img/RevIN.png" alt=""><br>æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯æ•°æ®åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ï¼ˆdistribution shift problemï¼‰ï¼Œå³æ•°æ®åˆ†å¸ƒï¼Œæ¯”å¦‚æ•°æ®çš„å‡å€¼æ–¹å·®ç­‰ï¼Œä¼šéšç€æ—¶é—´è€Œå˜åŒ–ï¼Œè¿™ä¼šç»™æ—¶åºé¢„æµ‹é—®é¢˜é€ æˆä¸€å®šçš„éš¾åº¦ï¼ˆè¿™ç±»æ•°æ®ä¹Ÿæˆä¸ºéå¹³ç¨³æ•°æ® non-stationaryï¼‰ã€‚è€Œåœ¨æ—¶åºé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†å¾€å¾€æ˜¯æ—¶é—´æ¥åˆ’åˆ†çš„ï¼Œè¿™å¤©ç„¶ä¼šå¼•å…¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ­¤å¤–ï¼Œä¸åŒè¾“å…¥åºåˆ—ä¹Ÿä¼šæœ‰æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ä¸¤ä¸ªä¸ä¸€è‡´çš„é—®é¢˜éƒ½å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ•ˆæœçš„ä¸‹é™ã€‚</p><p>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¯ä»¥æƒ³åŠæ³•å»é™¤æ•°æ®ä¸­çš„éå¹³ç¨³ä¿¡æ¯ï¼Œä½†æ˜¯å¦‚æœåªæ˜¯ç®€å•çš„æ¶ˆé™¤éå¹³ç¨³ä¿¡æ¯ï¼Œä¼šå¯¼è‡´éå¹³ç¨³ä¿¡æ¯ä¸¢å¤±ï¼Œè¿™å¯èƒ½ä¼šå½±å“åˆ°æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°è¿™éƒ¨åˆ†ä¿¡æ¯ï¼Œè¿›è€Œå½±å“åˆ°æ¨¡å‹æ•ˆæœã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡ºäº†åœ¨æ¨¡å‹è¾“å‡ºåæ˜¾å¼æ¢å¤éå¹³ç¨³ä¿¡æ¯çš„æ€è·¯ï¼Œè¿™æ ·æ—¢ä½¿æ¨¡å‹åœ¨å­¦ä¹ æ—¶å¿½ç•¥äº†æ•°æ®æ¼‚ç§»çš„é—®é¢˜ï¼Œåˆé¿å…äº†éå¹³ç¨³ä¿¡æ¯çš„ä¸¢å¤±ã€‚</p><p>æœ¬ç¯‡è®ºæ–‡æå‡ºçš„æ˜¯ä¸€ç§æ•°æ®è§„èŒƒåŒ–çš„æ–¹æ³•ï¼Œå‘½åä¸ºâ€œå¯é€†å®ä¾‹è§„èŒƒåŒ–â€ ï¼ˆreversible instance normalizationï¼ŒRevINï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒRevIN åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œè§„èŒƒåŒ–å’Œé€†è§„èŒƒåŒ–ï¼Œé¦–å…ˆåœ¨æ•°æ®è¾“å…¥æ¨¡å‹å‰ï¼Œå°†æ•°æ®è¿›è¡Œè§„èŒƒåŒ–ï¼Œç„¶åç»è¿‡æ¨¡å‹å­¦ä¹ åå¾—åˆ°æ¨¡å‹è¾“å‡ºï¼Œæœ€åå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåè§„èŒƒåŒ–ã€‚RevIN æ˜¯ä¸€ç§çµæ´»çš„ï¼Œç«¯åˆ°ç«¯çš„å¯è®­ç»ƒå±‚ï¼Œèƒ½å¤Ÿè¢«åº”ç”¨åˆ°ä»»æ„æ¨¡å‹å±‚ã€‚</p><h2 id="Code">Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RevIN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features: <span class="built_in">int</span>, eps=<span class="number">1e-5</span>, affine=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param num_features: the number of features or channels</span></span><br><span class="line"><span class="string">        :param eps: a value added for numerical stability</span></span><br><span class="line"><span class="string">        :param affine: if True, RevIN has learnable affine parameters</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(RevIN, self).__init__()</span><br><span class="line">        self.num_features = num_features</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.affine = affine</span><br><span class="line">        <span class="keyword">if</span> self.affine:</span><br><span class="line">            self._init_params()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mode:<span class="built_in">str</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;norm&#x27;</span>:</span><br><span class="line">            self._get_statistics(x)</span><br><span class="line">            x = self._normalize(x)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&#x27;denorm&#x27;</span>:</span><br><span class="line">            x = self._denormalize(x)</span><br><span class="line">        <span class="keyword">else</span>: <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_params</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># initialize RevIN params: (C,)</span></span><br><span class="line">        self.affine_weight = nn.Parameter(torch.ones(self.num_features))</span><br><span class="line">        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_statistics</span>(<span class="params">self, x</span>):</span><br><span class="line">        dim2reduce = <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="number">1</span>, x.ndim-<span class="number">1</span>))</span><br><span class="line">        self.mean = torch.mean(x, dim=dim2reduce, keepdim=<span class="literal">True</span>).detach()</span><br><span class="line">        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>) + self.eps).detach()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_normalize</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x - self.mean</span><br><span class="line">        x = x / self.stdev</span><br><span class="line">        <span class="keyword">if</span> self.affine:</span><br><span class="line">            x = x * self.affine_weight</span><br><span class="line">            x = x + self.affine_bias</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_denormalize</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.affine:</span><br><span class="line">            x = x - self.affine_bias</span><br><span class="line">            x = x / (self.affine_weight + self.eps*self.eps)</span><br><span class="line">        x = x * self.stdev</span><br><span class="line">        x = x + self.mean</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimMTM:A Simple Pre-Training Framework for Masked Time-Series Modeling</title>
      <link href="/paper%20notes/SimMTM/"/>
      <url>/paper%20notes/SimMTM/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #NeurIPS 2023<br>authors: Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, Mingsheng Long<br>code:</p></div><p><img src="/img/SimMTM.png" alt=""></p><blockquote><p>åŸå§‹åºåˆ—ç”Ÿæˆå¤šä¸ª mask ååºåˆ—ã€‚encoderï¼Œprojectorã€‚series-wise similarity å¯¹ point-wiserepresentation è¿›è¡ŒåŠ æƒåˆ†é…ã€‚åŠ æƒåˆ†é…åçš„ zâ€™decoderï¼Œé‡æ„ xâ€™ã€‚é‡æ„ loss(x, xâ€™)ï¼ŒåŒæ—¶å¯¹å…¶è¿›è¡Œçº¦æŸï¼ŒåŒä¸€ä¸ªåºåˆ—è¡¨å¾ä¸å…¶ mask åçš„å¤šä¸ªåºåˆ—çš„è¡¨å¾ä¸ºæ­£æ ·æœ¬ï¼Œå°½å¯èƒ½é è¿‘ã€‚åŒä¸€ä¸ª batch ä¸­çš„å…¶ä»–åºåˆ—è¡¨å¾åŠå…¶ mask åçš„å¤šä¸ªåºåˆ—çš„è¡¨å¾ä¸ºè´Ÿæ ·æœ¬ï¼Œå°½å¯èƒ½è¿œç¦»ã€‚</p></blockquote><p><img src="/img/SimMTM-1.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeurIPS </tag>
            
            <tag> é¢„è®­ç»ƒ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A TIME SERIES IS WORTH 64 WORDS:LONG-TERM FORECASTING WITH TRANSFORMERS</title>
      <link href="/paper%20notes/PatchTST/"/>
      <url>/paper%20notes/PatchTST/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICLR 2023<br>authors: Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam<br>code: <a href="https://github.com/yuqinie98/patchtst">https://github.com/yuqinie98/patchtst</a></p></div><p><img src="/img/PatchTST-1.png" alt=""></p><p>é€šé“ç‹¬ç«‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input (bs, c_nums, T)</span></span><br><span class="line"><span class="comment"># input.reshape(bs*c_nums, -1, T)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#code</span></span><br><span class="line"><span class="comment"># x: [bs x nvars x patch_num x d_model]</span></span><br><span class="line">u = torch.reshape(x, (x.shape[<span class="number">0</span>]*x.shape[<span class="number">1</span>],x.shape[<span class="number">2</span>],x.shape[<span class="number">3</span>]))      </span><br><span class="line"><span class="comment"># u: [bs * nvars x patch_num x d_model]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TS2Vec:Towards Universal Representation of Time Series</title>
      <link href="/paper%20notes/TS2Vec/"/>
      <url>/paper%20notes/TS2Vec/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #AAAI 2022<br>authors: Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, Bixiong Xu<br>code: <a href="https://github.com/yuezhihan/ts2vec">https://github.com/yuezhihan/ts2vec</a></p></div><h2 id="1-Introduction-And-Conclusion">1 <strong>Introduction And Conclusion</strong></h2><h3 id="1-1-Problems">1.1 <strong>Problems</strong></h3><ol><li>å¤§éƒ¨åˆ†ç ”ç©¶è¿›è¡Œçš„æ˜¯å®ä¾‹çº§åˆ«çš„è¡¨å¾ï¼ˆinstance-level representationsï¼‰ï¼Œä½†å®ä¾‹çº§è¡¨ç¤ºå¯èƒ½ä¸é€‚åˆéœ€è¦ç»†ç²’åº¦è¡¨ç¤ºçš„ä»»åŠ¡ (e.g. ts foresting &amp; ts AD)ã€‚</li><li>å¾ˆå°‘æœ‰æ–¹æ³•åœ¨ä¸åŒç²’åº¦ä¸ŠåŒºåˆ†å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li><li>å¤§å¤šæ•°æ—¶é—´åºåˆ—è¡¨ç¤ºæ–¹æ³•çš„çµæ„Ÿæ¥è‡ª CV å’Œ NLP é¢†åŸŸçš„ç»éªŒï¼Œè¿™äº›é¢†åŸŸå…·æœ‰å¾ˆå¼ºçš„å½’çº³åå·®ï¼Œè¿™äº›å‡è®¾å¹¶ä¸æ€»é€‚ç”¨äºæ—¶é—´åºåˆ—ã€‚</li></ol><h3 id="1-2-Contributions"><strong>1.2 Contributions</strong></h3><ol><li>TS2Vec, a unified framework that learns contextual representations for arbitrary sub-series at various semantic levels.</li><li>hierarchical contrasting method in instance-wise and temporal dimensions</li><li>contextual consistencyï¼ˆæ­£æ ·æœ¬çš„é€‰æ‹©ï¼‰</li></ol><h2 id="2-Method">2 <strong>Method</strong></h2><p><img src="/img/TS2Vec.png" alt="Architecture of TS2Vec"></p><h3 id="è¾“å…¥-input-N-x-T-x-F">è¾“å…¥ input: N x T x F</h3><blockquote><p>random sample two overlapping subseries at time dimension. X1(N x T1 x F), T1=[a1, b1]; X2(N x T2 x F), T2=[a2, b2]. 0â‰¤a1â‰¤a2â‰¤b1â‰¤b2â‰¤T, the overlapping segment [a2, b1]</p></blockquote><h3 id="Input-Projection-Layer">Input Projection Layer</h3><blockquote><p>X1â†’Z1(N x T1 x K), X2â†’Z2(N x T1 x K)</p></blockquote><h3 id="Timestamp-marking">Timestamp marking</h3><blockquote><p>along time axis with binary mask, Bernoulli distribution.</p></blockquote><h3 id="Dialated-Conv">Dialated Conv</h3><p>10 layer, and 2 Conv per layer.</p><p><img src="/img/TS2Vec-1.png" alt=""></p><h3 id="Hierarchical-contrasting">Hierarchical contrasting</h3><p><img src="/img/TS2Vec-2.png" alt=""></p><h4 id="Temporal-Contrastive-Loss">Temporal Contrastive Loss</h4><p>æ­£è´Ÿæ ·æœ¬å®šä¹‰ï¼šæ¥è‡ªåŒä¸€ä¸ªå®ä½“ (i.e. i x T1` x K &amp; i x T2` x K) ä¸åŒæ—¶åˆ»ã€‚æ­£æ ·æœ¬ï¼Œthe same timestamp from two view. è´Ÿæ ·æœ¬ (2(Î©-1))ï¼Œdifferent timestamp.</p><p><img src="/img/TS2Vec-3.png" alt="Î© is the set of timestamp within the overlap. (i.e. a2, b1)"></p><h4 id="Instance-wise-Contrastive-Loss">Instance-wise Contrastive Loss</h4><p>æ­£è´Ÿæ ·æœ¬å®šä¹‰ï¼šæ¥è‡ªåŒä¸åŒå®ä½“åŒä¸€æ—¶åˆ»ã€‚æ­£æ ·æœ¬ï¼ŒåŒä¸€å®ä½“ç›¸åŒæ—¶åˆ» and from two view. è´Ÿæ ·æœ¬ (2(B-1))ï¼Œdifferent instance.</p><p><img src="/img/TS2Vec-4.png" alt="B is the batch size"></p><h4 id="Overall-Loss">Overall Loss</h4><p><img src="/img/TS2Vec-5.png" alt=""></p><p>å¯¹äºæ¯ä¸€ä¸ªå±‚æ¬¡ï¼Œè¿­ä»£è®¡ç®—</p><p><img src="/img/TS2Vec-6.png" alt=""></p><h2 id="3-Experiments"><strong>3 Experiments</strong></h2><h3 id="Time-Series-Classification">Time Series Classification</h3><p><img src="/img/TS2Vec-7.png" alt="Critical Difference (CD) diagram of representation learning methods on time series classification tasks with a confidence level of 95%"></p><h3 id="Time-Series-Forecasting">Time Series Forecasting</h3><p><img src="/img/TS2Vec-8.png" alt="Univariate time series forecasting results on MSE"></p><p><img src="/img/TS2Vec-9.png" alt="The running time (in seconds) comparison on multivariate forecasting task on ETTm1 dataset"></p><h3 id="Time-Series-Anomaly-Detection">Time Series Anomaly Detection</h3><p><img src="/img/TS2Vec-10.png" alt="Univariate time series anomaly detection results"></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é¢„è®­ç»ƒ </tag>
            
            <tag> å¯¹æ¯”å­¦ä¹  </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCdetector-Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection</title>
      <link href="/paper%20notes/DCdetector/"/>
      <url>/paper%20notes/DCdetector/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #KDD 2023<br>authors: Yiyuan Yang, Chaoli Zhang, Tian Zhou<br>code: <a href="https://github.com/DAMO-DI-ML/KDD2023-DCdetector">https://github.com/DAMO-DI-ML/KDD2023-DCdetector</a></p></div><p><img src="/img/DCdetector-2.png" alt=""></p><h2 id="1-Introduction-And-Conclusion">1 <strong>Introduction And Conclusion</strong></h2><p>è¿™ç¯‡æ–‡ç« çš„å½’çº³åç½®å’Œ Anomaly Transformer ç›¸ä¼¼ã€‚</p><blockquote><p>å¼‚å¸¸ç‚¹å’Œæ•´ä¸ªåºåˆ—å…³è”å°‘ï¼ˆå°‘è§ï¼‰ï¼Œå’Œä¸´è¿‘çš„ç‚¹å…³è”ç›¸å¯¹å¤šï¼›è€Œæ­£å¸¸ç‚¹å¯èƒ½å…±äº«ä¸€äº›æ½œåœ¨çš„æ¨¡å¼ï¼Œä¸å…¶ä»–ç‚¹çš„å…³è”ç›¸å¯¹å¼ºã€‚</p></blockquote><p>Anomaly Transformer é€šè¿‡å¯å­¦ä¹ é«˜æ–¯æ ¸å’Œæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒçš„å…³è”å·®å¼‚ï¼ˆå·®å¼‚å°ï¼Œæ³¨æ„åŠ›é›†ä¸­åœ¨å±€éƒ¨ï¼Œæ›´å¯èƒ½æ˜¯å¼‚å¸¸ï¼‰æ¥æ£€æµ‹å¼‚å¸¸ã€‚å¯¹æ¯”äº Anomaly Transformerï¼Œè¿™ç¯‡æ–‡ç« é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•å®ç°äº†ç±»ä¼¼çš„ç›®æ ‡ã€‚</p><p><img src="/img/DCdetector-3.png" alt=""></p><h3 id="1-1-Problems">1.1 <strong>Problems</strong></h3><blockquote><p>â“å¸¸è§çš„å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œa. It takes work to get tables. b. éœ€è¦è€ƒè™‘æ—¶é—´ä¾èµ–ï¼Œå¤šç»´åº¦é—´ä¾èµ–å’Œéç»Ÿè®¡ç‰¹å¾ã€‚c. å¼‚å¸¸å°‘è§<br>åŸºäºé‡æ„çš„æ–¹æ³•ï¼Œåœ¨ä¸å—å¼‚å¸¸é˜»ç¢çš„æƒ…å†µä¸‹å­¦ä¹ æ­£å¸¸æ•°æ®çš„è‰¯å¥½é‡æ„æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ¢è¨€ä¹‹ï¼Œå­¦ä¹ ä¸€ä¸ªå¹²å‡€çš„ï¼Œå¯ä»¥å¾ˆå¥½é‡æ„æ­£å¸¸ç‚¹çš„æ¨¡å‹å¾ˆå›°éš¾</p></blockquote><h3 id="1-2-Contributions"><strong>1.2 Contributions</strong></h3><blockquote><p>ğŸ’¡æå‡ºäº†åŸºäºåŒé‡æ³¨æ„åŠ›çš„å¯¹æ¯”å­¦ä¹ ç»“æ„ï¼ˆdual-branch attention)ã€é€šé“ç‹¬ç«‹ï¼Œå¤šå°ºåº¦ã€‘<br>è®­ç»ƒåªéœ€è¦å¯¹æ¯”ï¼Œè€Œä¸éœ€è¦é‡æ„è¯¯å·®ï¼ˆå’Œ Anomaly Transformer æ¯”è¾ƒï¼‰</p></blockquote><h2 id="2-Method">2 <strong>Method</strong></h2><p><img src="/img/DCdetector-2.png" alt=""></p><h3 id="2-1-é€šé“ç‹¬ç«‹â†’patching">2.1 é€šé“ç‹¬ç«‹â†’patching</h3><p><img src="/img/DCdetector.png" alt=""></p><h3 id="2-2-patch-wise-attentionï¼Œpatch-in-attention-ã€‚ï¼ˆ-ä¸Šé‡‡æ ·-å¤šå°ºåº¦ï¼‰">2.2 <strong>patch-wise attention</strong>ï¼Œpatch-in attention**ã€‚ï¼ˆ+**ä¸Šé‡‡æ · +<strong>å¤šå°ºåº¦ï¼‰</strong></h3><p><img src="/img/DCdetector-4.png" alt=""></p><h4 id="2-2-1-Attention">2.2.1 Attention</h4><p>patch-wiseï¼Œpatch å’Œ patch ä¹‹é—´ï¼ˆP x N x d -&gt; N x d)ï¼›patch-inï¼Œpatch å†…éƒ¨ï¼ˆP x N x d -&gt; P xdï¼‰ã€‚è§ä¸Šç¤ºæ„å›¾ã€‚å¯¹äºæŸä¸ªæ—¶åˆ»çš„ç‚¹æ¥è¯´ï¼Œpatch-wise å°±æ˜¯å»è®¡ç®—å®ƒä¸å…¶ä»–å‡ ä¸ª patch ç›¸åŒä½ç½®çš„ attentionï¼Œpatch-in å°±æ˜¯è®¡ç®—åŒä¸€ä¸ª patch å†…å®ƒä¸å…¶ä»–ç‚¹çš„ attentionã€‚</p><p>Wq å’Œ Wk å‚æ•°å…±äº«æƒé‡ã€‚</p><h4 id="2-2-2-ä¸Šé‡‡æ ·">2.2.2 ä¸Šé‡‡æ ·</h4><p>å°†ä»–ä»¬çš„å¤§å°è°ƒæ•´åˆ°ä¸€è‡´</p><p><img src="/img/DCdetector-5.png" alt=""></p><h4 id="2-2-3-å¤šå°ºåº¦">2.2.3 å¤šå°ºåº¦</h4><p>æ˜¯æŒ‡ patch çš„å¤§å°ä¸åŒï¼Œæœ€åæ¯ä¸ªä¸åŒçš„ patch size ç›¸åŠ ã€‚</p><p><img src="/img/DCdetector-6.png" alt=""></p><p>å¯ä»¥å°†è¿™ä¸¤ç§è¡¨ç¤ºè§†ä¸ºæ’åˆ—çš„å¤šè§†å›¾è¡¨ç¤º (e.g. aabbcc -&gt; abcabc)ã€‚å½’çº³åç½®ï¼šæ­£å¸¸ç‚¹å¯ä»¥åœ¨æ’åˆ—ä¸‹ä¿æŒå…¶è¡¨ç¤ºï¼Œè€Œå¼‚å¸¸ç‚¹åˆ™ä¸èƒ½ã€‚å¸Œæœ›ä»è¿™ç§å¯¹æ¯”å­¦ä¹ ä¸­å­¦ä¹ ä¸€ç§æ’åˆ—ä¸å˜çš„è¡¨ç¤ºã€‚</p><h3 id="2-3-æŸå¤±å‡½æ•°ï¼ˆKLæ•£åº¦ï¼‰">2.3 æŸå¤±å‡½æ•°ï¼ˆ<strong>KL</strong>æ•£åº¦ï¼‰</h3><p><img src="/img/DCdetector-7.png" alt=""></p><p><img src="/img/DCdetector-8.png" alt=""></p><h2 id="3-Experiments"><strong>3 Experiments</strong></h2><p><img src="/img/DCdetector-9.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¼‚å¸¸æ£€æµ‹ </tag>
            
            <tag> å¯¹æ¯”å­¦ä¹  </tag>
            
            <tag> KDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time-Series Representation Learning via Temporal and Contextual Contrasting</title>
      <link href="/paper%20notes/TS-TCC/"/>
      <url>/paper%20notes/TS-TCC/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #IJCAI 2021<br>authors: Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li and Cuntai Guan<br>code:</p></div><p>1. <strong>Introduction and Conclusion</strong></p><p>TS-TCC: Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC)</p><p><strong>Problems</strong></p><blockquote><p>ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¸èƒ½å¾ˆå¥½çš„å¼ºè°ƒæ—¶åºçš„æ—¶é—´ä¾èµ–<br>ä¸€äº›é’ˆå¯¹æ–‡æœ¬çš„æ•°æ®å¢å¼ºæŠ€æœ¯ä¸é€‚ç”¨äºæ—¶åºæ•°æ®</p></blockquote><p><strong>Contributions</strong></p><blockquote><p>å¼ºå¼±æ•°æ®å¢å¼ºï¼ˆäº§ç”Ÿ 2 ä¸ªå¢å¼ºåçš„åºåˆ—ï¼‰<br>ä¸¤ä¸ªå¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼ˆtemporal contrasting and contextual contrastingï¼‰</p></blockquote><p>2. <strong>Method</strong></p><p>å¼ºå¼±å¢å¼º</p><p>In this paper, weak augmentation is a jitter-and-scale strategy. Specifically, we add random variations to the signal and scale up its magnitude. For strong augmentation, we apply permutation-and-jitter strategy, where permutation includes splitting the signal into a random number of segments with a maximum of M and randomly shuffling them. Next, a random jittering is added to the permuted signal.</p><blockquote><p>åœ¨æœ¬æ–‡ä¸­ï¼Œå¼±å¢å¼ºæ˜¯ä¸€ç§æŠ–åŠ¨å’Œç¼©æ”¾ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†éšæœºå˜åŒ–æ·»åŠ åˆ°ä¿¡å·ä¸­ï¼Œå¹¶æ”¾å¤§å…¶å¹…åº¦ã€‚å¯¹äºå¼ºå¢å¼ºï¼Œæˆ‘ä»¬åº”ç”¨ç½®æ¢å’ŒæŠ–åŠ¨ç­–ç•¥ï¼Œå…¶ä¸­ç½®æ¢åŒ…æ‹¬å°†ä¿¡å·æ‹†åˆ†ä¸ºæœ€å¤§å€¼ä¸º M çš„éšæœºæ•°ç›®çš„æ®µï¼Œå¹¶éšæœºæ…ä¹±å®ƒä»¬ã€‚æ¥ä¸‹æ¥ï¼Œå°†éšæœºæŠ–åŠ¨æ·»åŠ åˆ°ç»æ’åˆ—çš„ä¿¡å·ã€‚</p></blockquote><p><img src="/img/TS-TCC.png" alt=""></p><p>Tempporal contrasting å€Ÿé‰´äº† CPCï¼ˆä¸ CPC ä¸åŒä¹‹å¤„åœ¨äºæ­£æ ·æœ¬ä¸ºå¯¹åº”çš„å¦ä¸€ä¸ªåºåˆ—ä¸­çš„ Z_(t+k)ï¼Œè€Œ CPC åªæœ‰ä¸€ä¸ªåºåˆ—)</p><p><img src="/img/TS-TCC-1.png" alt=""></p><p>Contextual contrastingï¼Œä½¿ç”¨çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚æœ€å¤§åŒ–åŒä¸€æ ·æœ¬ä¸Šä¸‹æ–‡ï¼Œæœ€å°åŒ–å…¶ä»–æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬ï¼‰ä¸Šä¸‹æ–‡ã€‚</p><p><img src="/img/TS-TCC-2.png" alt=""></p><p><em>å¯¹äºå«æœ‰ N ä¸ªæ•°æ®çš„ä¸€æ‰¹æ•°æ®ï¼Œé€šè¿‡å¼ºå¼±æ•°æ®å¢å¼ºï¼Œäº§ç”Ÿçš„ä¸¤ä¸ªä¸åŒ view çš„åºåˆ—ï¼Œæ¯ä¸ªæ•°æ®éƒ½æœ‰ä¸Šä¸‹æ–‡ c ç”Ÿæˆï¼ˆ2Nï¼‰ï¼Œi.e.å¯¹äºå½“å‰æŸä¸ª C_t(strong)ï¼Œå…¶æ­£æ ·æœ¬æ˜¯å¦ä¸€ä¸ªæ•°æ®å¢å¼ºååºåˆ—çš„å¯¹åº” C_t(weak)ï¼Œå…¶ä½™ 2N-2 ä¸ªä¸ºè´Ÿæ ·æœ¬ã€‚</em></p><p><img src="/img/TS-TCC-3.png" alt=""></p><p>3. <strong>Experiments</strong></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¯¹æ¯”å­¦ä¹  </tag>
            
            <tag> IJCAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Representation Learning with Contrastive Predictive Coding</title>
      <link href="/paper%20notes/CPC/"/>
      <url>/paper%20notes/CPC/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference:<br>authors: Aaron van den Oord, Yazhe Li, Oriol Vinyals<br>code:</p></div>  <p><img src="/img/CPC.png" alt=""></p><h2 id="NCE">NCE</h2><p><a href="https://www.zhihu.com/question/50043438" title="æ±‚é€šä¿—æ˜“æ‡‚è§£é‡Šä¸‹nce lossï¼Ÿ - çŸ¥ä¹ (zhihu.com)">æ±‚é€šä¿—æ˜“æ‡‚è§£é‡Šä¸‹nce lossï¼Ÿ - çŸ¥ä¹ (zhihu.com)</a></p><p><a href="https://spaces.ac.cn/archives/5617" title="â€œå™ªå£°å¯¹æ¯”ä¼°è®¡â€æ‚è°ˆï¼šæ›²å¾„é€šå¹½ä¹‹å¦™ - ç§‘å­¦ç©ºé—´|Scientific Spaces">â€œå™ªå£°å¯¹æ¯”ä¼°è®¡â€æ‚è°ˆï¼šæ›²å¾„é€šå¹½ä¹‹å¦™ - ç§‘å­¦ç©ºé—´|Scientific Spaces</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¯¹æ¯”å­¦ä¹  </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Not All Features Matter-Enhancing Few-shot CLIP with Adaptive Prior Refinement</title>
      <link href="/paper%20notes/CLIP_Adaptive%20Prior%20Refinement/"/>
      <url>/paper%20notes/CLIP_Adaptive%20Prior%20Refinement/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICCV 2023<br>authors: Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, Peng Gao<br>code: <a href="https://github.com/yangyangyang127/ape">https://github.com/yangyangyang127/ape</a></p></div><p><img src="/img/Not%20All%20Features%20Matter-%20Enhancing%20Few-shot%20CLIP%20with%20Adaptive%20Prior%20Refinement.png" alt=""></p><p><img src="/img/Not%20All%20Features%20Matter-%20Enhancing%20Few-shot%20CLIP%20with%20Adaptive%20Prior%20Refinement-1.png" alt=""></p><blockquote><p>åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œè€ƒè™‘åˆ°æ•°æ®åˆ†å¸ƒï¼ŒCLIP æå–çš„ç‰¹å¾å¯èƒ½å¹¶ä¸å…¨æ˜¯æœ‰ç”¨çš„ï¼Œå› æ­¤ä½œè€…è¯•å›¾ä¸ºæ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†æçº¯ä¸ªæ€§åŒ–çš„ç‰¹å¾ã€‚é€šè¿‡æœ€å¤§åŒ–ç±»é—´å·®å¼‚ï¼Œæˆ–è€…è¯´æœ€å°åŒ–ç±»é—´ç›¸ä¼¼åº¦ï¼Œæ¥é€‰æ‹©åˆé€‚çš„ç‰¹å¾ã€‚å¯¹äºä¸€ä¸ªæœ‰ C ä¸ªç±»çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè®¡ç®—æ‰€æœ‰ç±»çš„æ‰€æœ‰æ ·æœ¬è¡¨å¾ä¹‹é—´å¹³å‡ç›¸ä¼¼åº¦ã€‚ç„¶è€Œï¼Œä¸ºæ•´ä¸ªæ•°æ®é›†ï¼ˆå³ä½¿æ˜¯å°‘æ•°æ ·æœ¬ï¼‰è®¡ç®— S çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œè€ƒè™‘åˆ° CLIP çš„å¯¹æ¯”é¢„è®­ç»ƒï¼Œå…¶ä¸­è§†è§‰è¯­è¨€è¡¨ç¤ºå·²ç»å¾ˆå¥½åœ°å¯¹é½ï¼Œä¸‹æ¸¸ç±»åˆ«çš„æ–‡æœ¬ç‰¹å¾å¯ä»¥è¢«è§†ä¸ºä¸€ç»„è§†è§‰åŸå‹ï¼Œè¿™æ ·çš„åŸå‹å¯ä»¥è¿‘ä¼¼ä¸åŒç±»åˆ«çš„è§†è§‰ç‰¹å¾åœ¨åµŒå…¥ç©ºé—´ä¸­çš„èšç±»ä¸­å¿ƒã€‚æ‰€ä»¥ä½œè€…åˆ©ç”¨æ¨¡æ¿â€œa photo of a [CLASS]â€å¹¶å°†æ‰€æœ‰ç±»åˆ«åç§°æ”¾å…¥ [CLASS] ä½œä¸º CLIP çš„è¾“å…¥ã€‚ç„¶åæˆ‘ä»¬å°†ä¸‹æ¸¸ç±»åˆ«çš„æ–‡æœ¬ç‰¹å¾è¡¨ç¤ºä¸º xiã€‚åŒæ—¶å› ä¸º xi å·²ç»ç»è¿‡ L2 æ ‡å‡†åŒ–ï¼Œæ¯ä¸€ä¸ªé€šé“é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¯ç®€å•è¡¨ç¤ºä¸º xik ä¸ xjk çš„ä¹˜ç§¯ã€‚Sk å°±è¡¨ç¤ºç¬¬ k ä¸ªé€šé“çš„å¹³å‡ç±»é—´ç›¸ä¼¼åº¦ã€‚åŒç†ï¼Œç¬¬ k ä¸ªé€šé“çš„ç±»é—´æ–¹å·®å¯ä»¥ç”¨ Vk è¡¨ç¤ºï¼Œå…¶ä¸­ xk-bar è¡¨ç¤ºç¬¬ k ä¸ªé€šé“çš„å¹³å‡å€¼ã€‚ä½œè€…ä½¿ç”¨ Vk æ¥æ¶ˆé™¤ç±»åˆ«ä¹‹é—´å‡ ä¹ä¿æŒä¸å˜çš„ç‰¹å¾é€šé“ï¼Œè¿™äº›ç‰¹å¾é€šé“æ²¡æœ‰è¡¨ç°å‡ºç±»é—´å·®å¼‚ï¼Œå¯¹åˆ†ç±»å½±å“å¾ˆå°ã€‚ä½œè€…æ•´ä½“çš„ç›®æ ‡æ˜¯å¸Œæœ›æœ€å°åŒ–ç±»é—´ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶å»é™¤ç±»åˆ«é—´å‡ ä¹ä¿æŒä¸å˜çš„ç‰¹å¾é€šé“ã€‚æœ€åï¼Œä½œè€…å°†ç›¸ä¼¼æ€§å’Œæ–¹å·®æ ‡å‡†ä¸å¹³è¡¡å› å­ Î» æ··åˆä½œä¸ºæœ€ç»ˆé‡ã€‚é€‰æ‹©æœ€å°çš„ Q ä¸ª Jk ä½œä¸ºæœ€ç»ˆçš„ç»†åŒ–ç‰¹å¾é€šé“ï¼Œè¿™è¡¨æ˜ç±»é—´å·®å¼‚å’ŒåŒºåˆ†åº¦æœ€å¤§ã€‚å›¾æ˜¾ç¤ºäº†è‡ªé€‚åº”ç»†åŒ–æ¨¡å—å¸¦æ¥çš„å¥½å¤„ã€‚å¯¹äºç»†åŒ–åçš„ 512 ä¸ªç‰¹å¾é€šé“ï¼Œå›¾åƒä¹‹é—´çš„ç±»é—´ç›¸ä¼¼åº¦ï¼ˆâ€˜Inter-class Image-Imageâ€™ï¼‰å·²å¤§å¤§é™ä½ï¼Œè¡¨æ˜ç±»åˆ«åŒºåˆ†åº¦è¾ƒå¼ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„ç»†åŒ–æ›´å¥½åœ°å¯¹é½äº†é…å¯¹çš„å›¾åƒæ–‡æœ¬ç‰¹å¾ï¼ˆâ€œMatched ImageTextâ€ï¼‰ï¼Œå¹¶æ’é™¤äº†æœªé…å¯¹çš„ç‰¹å¾ï¼ˆâ€œUnmatched Image-Textâ€ï¼‰ï¼Œä»è€Œå¢å¼ºäº† CLIP ç”¨äºä¸‹æ¸¸è¯†åˆ«çš„å¤šæ¨¡æ€å¯¹åº”æ€§ã€‚</p></blockquote><p><img src="/img/Not%20All%20Features%20Matter-%20Enhancing%20Few-shot%20CLIP%20with%20Adaptive%20Prior%20Refinement-2.png" alt=""></p><p><img src="/img/Not%20All%20Features%20Matter-%20Enhancing%20Few-shot%20CLIP%20with%20Adaptive%20Prior%20Refinement-3.png" alt=""></p><blockquote><p>åœ¨æœ€åçš„å¼å­ä¸­ï¼Œå…¶ä¸­Î±ä½œä¸ºå¹³è¡¡å› å­ï¼Œdiag(Â·) è¡¨ç¤ºå¯¹è§’åŒ–ã€‚ç¬¬ä¸€é¡¹ä»£è¡¨ CLIP çš„é›¶æ ·æœ¬é¢„æµ‹ï¼Œå¹¶åŒ…å«å…¶é¢„å…ˆè®­ç»ƒçš„å…ˆéªŒçŸ¥è¯†ã€‚ç¬¬äºŒé¡¹è¡¨ç¤ºç¼“å­˜æ¨¡å‹çš„å°æ ·æœ¬é¢„æµ‹ï¼Œå®ƒåŸºäºç»†åŒ–åçš„ç‰¹å¾é€šé“å’Œ RF â€˜Wâ€™ çš„é‡æ–°æƒé‡è°ƒæ•´ã€‚</p></blockquote><blockquote><p>åœ¨éœ€è¦è®­ç»ƒ APE ä¸­ï¼Œç±»ä¼¼ä¸æ— éœ€è®­ç»ƒçš„ APEï¼Œåªä¸è¿‡é¢å¤–çš„åŠ å…¥äº†ä¸€ä¸ªè½»é‡çš„ç±»åˆ«æ®‹å·®ï¼Œå®ƒç”± C ä¸ªå¯å­¦ä¹ çš„åµŒå…¥ç»„æˆï¼Œæ¯ä¸ª embedding å¯¹åº”ä¸€ä¸ªä¸‹æ¸¸ç±»åˆ«ï¼Œæ—¨åœ¨å°‘æ ·æœ¬è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–ä¸åŒç±»åˆ«çš„ç»†åŒ–çš„ Q ä¸ªç‰¹å¾é€šé“ã€‚ä¸ºäº†ä¿ç•™åµŒå…¥ç©ºé—´ä¸­çš„è§†è§‰ - è¯­è¨€å¯¹åº”å…³ç³»ï¼Œå°† Res åŒæ—¶åº”ç”¨äºæ–‡æœ¬ç‰¹å¾ W å’Œè®­ç»ƒé›†ç‰¹å¾ Fâ€²ã€‚é€šè¿‡ä»…è®­ç»ƒæ­¤ç±»å°è§„æ¨¡å‚æ•°ï¼ŒAPE-T é¿å…äº†æ˜‚è´µçš„ç¼“å­˜æ¨¡å‹å¾®è°ƒï¼Œå¹¶é€šè¿‡æ›´æ–°ä¸¤ç§æ¨¡å¼çš„ç»†åŒ–ç‰¹å¾æ¥å®ç°å“è¶Šçš„æ€§èƒ½ã€‚</p></blockquote><blockquote><p>fW,è¡¨ç¤ºæµ‹è¯•å›¾åƒå’Œç±»åˆ«æ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒåŸå§‹çš„ CLIP é›¶æ ·æœ¬é¢„æµ‹ã€‚</p></blockquote><blockquote><p>fâ€™Fâ€™ è¡¨ç¤ºå…·æœ‰è°ƒåˆ¶æ ‡é‡Î² çš„ç¼“å­˜æ¨¡å‹çš„å›¾åƒ - å›¾åƒç›¸ä¼¼åº¦ã€‚å‚è€ƒåŸºäºå…ˆéªŒçš„æ–¹æ³•ã€‚</p></blockquote><blockquote><p>æ­¤å¤–ï¼Œä½œè€…è¿›ä¸€æ­¥è€ƒè™‘äº† Fâ€²å’Œ Wâ€²ä¹‹é—´çš„å…³ç³»ï¼Œè¡¨ç¤º CLIP å¯¹å°‘æ ·æœ¬è®­ç»ƒæ•°æ®çš„é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›ï¼Œä¸ºäº†è¯„ä¼°è¿™ç§è¯†åˆ«èƒ½åŠ›ï¼Œä½œè€…è®¡ç®—äº† CLIP å¯¹å°‘æ ·æœ¬é›†çš„é¢„æµ‹å’Œä»–ä»¬ one-hot æ ‡ç­¾ L ä¹‹é—´çš„å…³ç³»ï¼Œå…¶è¡¨ç¤ºä¸º KL æ•£åº¦ã€‚CLIP å¯¹æŸä¸ªå°‘æ ·æœ¬è®­ç»ƒé›†çš„ç±»é¢„æµ‹è¶Šå¥½ï¼Œè¶Šæ¥è¿‘çœŸå®çš„æ ‡ç­¾ï¼Œå…¶å€¼è¶Šå°ï¼Œé¢„æµ‹è¶Šå·®ï¼Œå€¼è¶Šå¤§ã€‚å…¶ä¸­Î³æ˜¯å¹³æ»‘å› å­ã€‚</p></blockquote><blockquote><p>Fâ€™W â€™ å¯ä»¥çœ‹ä½œæ˜¯ç¼“å­˜æ¨¡å‹ä¸­æ¯ä¸ªè®­ç»ƒç‰¹å¾çš„åˆ†æ•°ï¼Œè¡¨ç¤ºé€šè¿‡ CLIP æå–çš„è¡¨ç¤ºçš„ç²¾åº¦ï¼Œä»¥åŠå…¶å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®æœ‰å¤šå¤§ã€‚</p></blockquote><p>é€šè¿‡è‡ªé€‚åº”å…ˆéªŒç»†åŒ–å’Œä¸‰è¾¹å…³ç³»åˆ†æï¼ŒAPE å¯ä»¥é«˜æ•ˆä¸”æœ‰æ•ˆåœ°å¢å¼ºå°‘æ ·æœ¬ CLIPã€‚</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é¢„è®­ç»ƒ </tag>
            
            <tag> ICCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
      <link href="/paper%20notes/CycleGAN/"/>
      <url>/paper%20notes/CycleGAN/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #ICCV 2017<br>authors: Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros<br>code: <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p></div><p><img src="/img/CycleGAN.png" alt=""></p><p>ä¼ ç»Ÿ GAN æœ‰ä¸€ä¸ªç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼Œç”Ÿæˆå™¨ G ç”¨äºç”Ÿæˆæ ·æœ¬ï¼Œåˆ¤åˆ«å™¨ D ç”¨äºåˆ¤æ–­è¿™ä¸ªæ ·æœ¬æ˜¯å¦ä¸ºçœŸæ ·æœ¬ã€‚G ç”¨éšæœºå™ªå£°ç”Ÿæˆå‡å›¾ï¼ŒD æ ¹æ®çœŸå‡å›¾è¿›è¡ŒäºŒåˆ†ç±»çš„è®­ç»ƒã€‚D æ ¹æ®è¾“å…¥çš„å›¾åƒç”Ÿæˆ scoreï¼Œè¿™ä¸ª score è¡¨ç¤º G ç”Ÿæˆçš„å›¾åƒæ˜¯å¦æˆåŠŸï¼Œè¿›è€Œè¿›ä¸€æ­¥çš„è®­ç»ƒ G ç”Ÿæˆæ›´å¥½çš„å›¾åƒã€‚</p><p>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆç½‘ç»œ G çš„ç›®æ ‡å°±æ˜¯å°½é‡ç”ŸæˆçœŸå®çš„å›¾ç‰‡å»æ¬ºéª—åˆ¤åˆ«ç½‘ç»œ Dã€‚è€Œ D çš„ç›®æ ‡å°±æ˜¯å°½é‡æŠŠ G ç”Ÿæˆçš„å›¾ç‰‡å’ŒçœŸå®çš„å›¾ç‰‡åˆ†åˆ«å¼€æ¥ã€‚æ•´ä¸ªå¼å­ç”±ä¸¤é¡¹æ„æˆã€‚x è¡¨ç¤ºçœŸå®å›¾ç‰‡ï¼Œz è¡¨ç¤ºè¾“å…¥ G ç½‘ç»œçš„å™ªå£°ï¼Œè€Œ G(z) è¡¨ç¤º G ç½‘ç»œç”Ÿæˆçš„å›¾ç‰‡.D(x) è¡¨ç¤º D ç½‘ç»œåˆ¤æ–­çœŸå®å›¾ç‰‡æ˜¯å¦çœŸå®çš„æ¦‚ç‡ï¼ˆå› ä¸º x å°±æ˜¯çœŸå®çš„ï¼Œæ‰€ä»¥å¯¹äº D æ¥è¯´ï¼Œè¿™ä¸ªå€¼è¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ã€‚è€Œ D(G(z)) æ˜¯ D ç½‘ç»œåˆ¤æ–­ G ç”Ÿæˆçš„å›¾ç‰‡çš„æ˜¯å¦çœŸå®çš„æ¦‚ç‡ã€‚G çš„ç›®çš„ï¼šG å¸Œæœ›è‡ªå·±ç”Ÿæˆçš„å›¾ç‰‡â€œè¶Šæ¥è¿‘çœŸå®è¶Šå¥½â€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒG å¸Œæœ› D(G(z)) å°½å¯èƒ½å¾—å¤§ï¼Œè¿™æ—¶ V(D,G) ä¼šå˜å°ã€‚å› æ­¤å¼å­çš„æœ€å‰é¢çš„è®°å·æ˜¯ min_Gã€‚D çš„ç›®çš„ï¼šD çš„èƒ½åŠ›è¶Šå¼ºï¼ŒD(x) åº”è¯¥è¶Šå¤§ï¼ŒD(G(x)) åº”è¯¥è¶Šå°ã€‚è¿™æ—¶ V(D,G) ä¼šå˜å¤§ã€‚å› æ­¤å¼å­å¯¹äº D æ¥è¯´æ˜¯æ±‚æœ€å¤§ (max_D)ã€‚</p><p>CycleGAN å…¶å®å°±æ˜¯ä¸€ä¸ª Xâ†’Y å•å‘ GAN åŠ ä¸Šä¸€ä¸ª Yâ†’X å•å‘ GANã€‚ä¸¤ä¸ª GAN å…±äº«ä¸¤ä¸ªç”Ÿæˆå™¨ï¼Œç„¶åå„è‡ªå¸¦ä¸€ä¸ªåˆ¤åˆ«å™¨ï¼Œæ‰€ä»¥åŠ èµ·æ¥æ€»å…±æœ‰ä¸¤ä¸ªåˆ¤åˆ«å™¨å’Œä¸¤ä¸ªç”Ÿæˆå™¨ã€‚ä¸€ä¸ªå•å‘ GAN æœ‰ä¸¤ä¸ª lossï¼Œè€Œ CycleGAN åŠ èµ·æ¥æ€»å…±æœ‰å››ä¸ª lossã€‚</p><p>Dx: X&amp;F(y) ï¼Œåˆ¤åˆ«å™¨ Dx åˆ¤åˆ«åˆ°åº•æ˜¯çœŸ X è¿˜æ˜¯ F æ ¹æ® Y ç”Ÿæˆçš„ä¸ X åŒåˆ†å¸ƒçš„æ•°æ®</p><p>Dy:Y &amp; G(x)ï¼Œåˆ¤åˆ«å™¨ Dy åˆ¤åˆ«åˆ°åº•æ˜¯çœŸ Y è¿˜æ˜¯ G æ ¹æ® X ç”Ÿæˆçš„ä¸ Y åŒåˆ†å¸ƒçš„æ•°æ®</p><p>å°†å¯¹æŠ—æ€§æŸå¤±åº”ç”¨äºä¸¤ä¸ªæ˜ å°„å‡½æ•°ï¼Œ</p><p>å¾ªç¯ä¸€è‡´æ€§æŸå¤±ç›®çš„å°±æ˜¯æŒ‡ domainX é€šè¿‡ç”Ÿæˆå™¨ G åˆ° domainY åå†é€šè¿‡ç”Ÿæˆå™¨ F åå‘å›åˆ° domainX, é‡æ–°ç”Ÿæˆçš„ä¸åŸå…ˆçš„å·®å¼‚å°½å¯èƒ½å°ï¼ŒdomainY åˆ° domainX å†åˆ° domainY ä¹ŸåŒç†ï¼Œé˜²æ­¢ç”Ÿæˆå™¨ G ä¸ F ç›¸äº’çŸ›ç›¾ï¼Œå³ä¸¤ä¸ªç”Ÿæˆå™¨ç”Ÿæˆæ•°æ®ä¹‹åè¿˜èƒ½å˜æ¢å›æ¥è¿‘ä¼¼çœ‹æˆ X-&gt;Y-&gt;X</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> GAN </tag>
            
            <tag> è¿ç§»å­¦ä¹  </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/paper%20notes/CLIP/"/>
      <url>/paper%20notes/CLIP/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #PCML 2021<br>authors: Al ec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, andhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever<br>code:</p></div><h2 id="Learning-Transferable-Visual-Models-From-Natural-Language-Supervision">Learning Transferable Visual Models From Natural Language Supervision</h2><p><img src="/img/CLIP-2.png" alt=""></p><p>1. <strong>Introduction and Conclusion</strong></p><p>1.1 <strong>Problems</strong></p><p>Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision</p><p>1.2 <strong>Contributions</strong></p><p>|main contribution is studying its behavior at large scale. åˆ©ç”¨è‡ªç„¶è¯­è¨€çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä½œä¸ºç›‘ç£ä¿¡å·æ¥å­¦ä¹ è§†è§‰ç‰¹å¾ã€‚</p><p>1.3 <strong>Motivation</strong></p><p>èƒŒæ™¯ï¼šç›´æ¥ä»åŸå§‹æ–‡æœ¬ä¸­å­¦ä¹ çš„é¢„è®­ç»ƒæ–¹æ³•åœ¨è¿‡å»å‡ å¹´ä¸­å½»åº•æ”¹å˜äº† NLPï¼Œå®ç°äº†é›¶æ ·æœ¬è¿ç§»åˆ°ä¸‹æ¸¸æ•°æ®ã€‚æ¯”å¦‚ gpt-3 ä¸€ç±»çš„æ¨¡å‹ï¼Œå‡ ä¹ä¸éœ€è¦ç‰¹å®šäºæ•°æ®é›†çš„è®­ç»ƒæ•°æ®ã€‚è€Œå½“å‰çš„è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ¨¡å‹é€šå¸¸è¢«è®­ç»ƒç”¨äºé¢„æµ‹æœ‰é™çš„ç‰©ä½“ç±»åˆ«ï¼Œè¿™æ ·çš„æ¨¡å‹é€šå¸¸è¿˜éœ€è¦é¢å¤–çš„æ ‡æ³¨æ•°æ®æ¥å®Œæˆè®­ç»ƒæ—¶æœªæ›¾è§è¿‡çš„è§†è§‰â€œæ¦‚å¿µâ€ã€‚åœ¨ NLP ä¸­ï¼Œé¢„è®­ç»ƒçš„æ–¹æ³•ç›®å‰å·²ç»è¢«éªŒè¯å¾ˆæˆåŠŸï¼Œç›´æ¥ä»ç½‘ç»œæ–‡æœ¬ä¸­å­¦ä¹ çš„å¯æ‰©å±•<strong>é¢„è®­ç»ƒ</strong>æ–¹æ³•èƒ½å¦åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¸¦æ¥ç±»ä¼¼çš„çªç ´ï¼Ÿ</p><p>ä½¿ç”¨è‡ªç„¶è¯­è¨€å­¦ä¹ çš„æ–¹æ³•å¯ä»¥ä»äº’è”ç½‘ä¸Šå¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸­å­¦ä¹ ï¼›</p><p>ä¸å¤§å¤šæ•°æ— ç›‘ç£æˆ–è‡ªç›‘ç£çš„å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œä»è‡ªç„¶è¯­è¨€ä¸­å­¦ä¹ ä¸åªæ˜¯å­¦ä¹ ä¸€ä¸ªè¡¨å¾ï¼Œè€Œä¸”è¿˜å°†è¯¥è¡¨å¾ä¸è¯­è¨€è”ç³»èµ·æ¥ï¼Œä»è€Œå®ç°çµæ´»çš„ zero-shot learningã€‚</p><p>2. <strong>Method</strong></p><p>å·¥ä½œçš„æ ¸å¿ƒæ˜¯ä»è‡ªç„¶è¯­è¨€ä¸å›¾åƒé…å¯¹çš„ç›‘ç£ä¸­å­¦ä¹ æ„ŸçŸ¥</p><p>2.1 <strong>Creating a Sufficiently Large Dataset - 400 million (image, text) pairs</strong></p><p>2.2 <strong>Selecting an Efficient Pre-Training Method - contrastive representation learning</strong></p><p><img src="/img/CLIP-3.png" alt=""></p><p>2.3 <strong>Choosing and Scaling a Model</strong></p><p>ResNet50, Vision Transformer(ViT)</p><p>Transformer</p><p>2.4 <strong>Pre-training</strong></p><p>The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs.</p><p>ç”±äºæ•°æ®é›†å¾ˆå¤§ï¼Œå› æ­¤ä¸ç”¨æ‹…å¿ƒè¿‡æ‹Ÿåˆé—®é¢˜ï¼›</p><p>æ²¡æœ‰åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå®Œå…¨ä»é›¶å¼€å§‹è®­ç»ƒï¼›</p><p>æ²¡æœ‰ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼›</p><p>æ²¡æœ‰ä½¿ç”¨æ–‡æœ¬æ•°æ®å¢å¼ºï¼ˆè¿™é‡Œä¸»è¦æŒ‡ä»æ–‡æœ¬ä¸­é€‰å–ä¸€ä¸ªå¥å­ï¼‰ï¼Œå› ä¸ºæ•°æ®é›†ä¸­çš„æ–‡æœ¬åªæœ‰ä¸€ä¸ªå¥å­ï¼›</p><p>å›¾åƒæ•°æ®å¢å¼ºæ–¹é¢åªä½¿ç”¨äº†éšæœºè£å‰ªï¼›</p><p>æ¸©åº¦å‚æ•° t åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¹Ÿè¢«ä¼˜åŒ–ã€‚</p><p><img src="/img/CLIP-2.png" alt=""></p><p>3. <strong>Experiments</strong></p><p><strong>zero-shot transfer</strong></p><p>zero-shot åˆ†ç±»</p><p><img src="/img/CLIP.png" alt=""></p><p><img src="/img/CLIP-4.png" alt=""></p><p><img src="/img/CLIP-5.png" alt=""></p><p><img src="/img/CLIP-6.png" alt=""></p><p>4. <strong>Limitations</strong></p><p><img src="/img/CLIP-7.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> é¢„è®­ç»ƒ </tag>
            
            <tag> å¯¹æ¯”å­¦ä¹  </tag>
            
            <tag> PCML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</title>
      <link href="/paper%20notes/OmniAnomaly/"/>
      <url>/paper%20notes/OmniAnomaly/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #KDD 2019<br>authors: Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, Dan Pei<br>code: <a href="https://github.com/smallcowbaby/OmniAnomaly">https://github.com/smallcowbaby/OmniAnomaly</a></p></div><p><img src="/img/OmniAnomaly.png" alt=""></p><p><img src="/img/OmniAnomaly-1.png" alt=""></p><p><img src="/img/OmniAnomaly-2.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¼‚å¸¸æ£€æµ‹ </tag>
            
            <tag> KDD </tag>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Triformer:Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecastingâ€“Full Version</title>
      <link href="/paper%20notes/Triformer/"/>
      <url>/paper%20notes/Triformer/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference:<br>authors: Ran-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, Shirui Pan<br>code:</p></div>  <p><strong>Introduction and Conclusion</strong></p><p>è§„èŒƒçš„ self attention å…·æœ‰äºŒæ¬¡å¤æ‚åº¦ã€‚å› æ­¤æ•ˆç‡è¾ƒä½ã€‚ &amp;# x20;</p><p>ç°æœ‰ç ”ç©¶ç”±äºå¯¹æ‰€æœ‰å˜é‡çš„æ—¶é—´åºåˆ—ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å‚æ•°ç©ºé—´ (å¦‚æŠ•å½±çŸ©é˜µ)ï¼Œå› æ­¤å‡†ç¡®æ€§ä¸è¶³ã€‚</p><p>Triformerï¼š</p><p>1. çº¿æ€§å¤æ‚åº¦ï¼špatch attentionï¼Œ ä¸º patch å¼•å…¥ä¼ªæ—¶é—´æˆ³<br>2. Variable-specific å‚æ•°ï¼šæå‡ºä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œä½¿ä¸åŒå˜é‡çš„æ—¶é—´åºåˆ—å…·æœ‰ä¸åŒçš„æ¨¡å‹å‚æ•°é›†</p><p><img src="/img/Triformer.png" alt=""></p><p><strong>Limitations</strong></p><p>å¯å­¦ä¹ å‚æ•° (pseudo-timestamps) çš„æ•°é‡ä¸é¢„æµ‹è®¾ç½®æœ‰å…³ï¼Œåœ¨åŠ¨æ€è¾“å…¥é•¿åº¦çš„åº”ç”¨ä¸­ä½¿ç”¨å—é™ã€‚</p><p><em>å°† patch çš„æ•°é‡ä¸ä¼ªæ—¶é—´æˆ³çš„æ•°é‡è§£è€¦ï¼ˆå¼•å…¥ç”Ÿæˆå™¨ï¼‰</em></p><p>cannot forecast different horizons once trained.</p><p><em>encoder-decoder architecture</em></p><p><strong>Method</strong><br><img src="/img/Triformer-1.png" alt=""></p><p><img src="/img/Triformer-2.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</title>
      <link href="/paper%20notes/Autoformer/"/>
      <url>/paper%20notes/Autoformer/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #NeurIPS 2021<br>authors: Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long<br>code:</p></div><p>ä¹‹å‰åŸºäº Transformer çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆself-attentionï¼‰æ¥æ•æ‰æ—¶åˆ»é—´çš„ä¾èµ–ï¼Œåœ¨æ—¶åºé¢„æµ‹ä¸Šå–å¾—äº†ä¸€äº›è¿›å±•ã€‚ä½†æ˜¯åœ¨é•¿æœŸåºåˆ—é¢„æµ‹ä¸­ï¼Œä»å­˜åœ¨ä¸è¶³ï¼š</p><ul><li>é•¿åºåˆ—ä¸­çš„å¤æ‚æ—¶é—´æ¨¡å¼ä½¿å¾—<strong>æ³¨æ„åŠ›æœºåˆ¶éš¾ä»¥å‘ç°å¯é çš„æ—¶åºä¾èµ–</strong>ã€‚</li><li>åŸºäº Transformer çš„æ¨¡å‹ä¸å¾—ä¸ä½¿ç”¨<strong>ç¨€ç–å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶</strong>æ¥åº”å¯¹äºŒæ¬¡å¤æ‚åº¦çš„é—®é¢˜ï¼Œä½†é€ æˆäº†<strong>ä¿¡æ¯åˆ©ç”¨çš„ç“¶é¢ˆ</strong>ã€‚</li></ul><p><img src="/img/Autoformer.png" alt=""></p><p>ä¸ºçªç ´ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬å…¨é¢é©æ–°äº† Transformerï¼Œå¹¶æå‡ºäº†åä¸º Autoformer çš„æ¨¡å‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹åˆ›æ–°ï¼š</p><ul><li>çªç ´å°†åºåˆ—åˆ†è§£ä½œä¸ºé¢„å¤„ç†çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡º<strong>æ·±åº¦åˆ†è§£æ¶æ„ï¼ˆDecomposition Architectureï¼‰</strong>ï¼Œèƒ½å¤Ÿä»å¤æ‚æ—¶é—´æ¨¡å¼ä¸­åˆ†è§£å‡ºå¯é¢„æµ‹æ€§æ›´å¼ºçš„ç»„åˆ†ã€‚</li><li>åŸºäºéšæœºè¿‡ç¨‹ç†è®ºï¼Œæå‡º<strong>è‡ªç›¸å…³æœºåˆ¶ï¼ˆAuto-Correlation Mechanismï¼‰</strong>ï¼Œä»£æ›¿ç‚¹å‘è¿æ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°åºåˆ—çº§ï¼ˆseries-wiseï¼‰è¿æ¥å’Œå¤æ‚åº¦ï¼Œæ‰“ç ´ä¿¡æ¯åˆ©ç”¨ç“¶é¢ˆã€‚</li></ul><p>åœ¨é•¿æœŸé¢„æµ‹é—®é¢˜ä¸­ï¼ŒAutoformer åœ¨èƒ½æºã€äº¤é€šã€ç»æµã€æ°”è±¡ã€ç–¾ç—…äº”å¤§æ—¶åºé¢†åŸŸå¤§å¹…è¶…è¶Šä¹‹å‰ SOTAï¼Œå®ç°<strong>38%</strong> çš„ç›¸å¯¹æ•ˆæœæå‡ã€‚</p><p><img src="/img/Autoformer-1.png" alt=""></p><h2 id="åˆ†è§£">åˆ†è§£</h2><p>å¯¹è¾“å…¥åºåˆ—è¿›è¡Œåˆ†è§£ï¼Œåˆ†è§£ä¸ºå­£èŠ‚æ€§éƒ¨åˆ†å’Œè¶‹åŠ¿éƒ¨åˆ†ã€‚ï¼ˆè¶‹åŠ¿ï¼ŒçŸ­æœŸæ³¢åŠ¨ï¼›å­£èŠ‚æ€§ï¼šé•¿æœŸï¼‰<br><strong>moving average</strong></p><h2 id="Auto-correlation">Auto-correlation</h2><blockquote><p>ç†è§£ï¼šå¯¹äºé•¿åº¦ä¸º a çš„æ—¶å»¶ï¼Œå¯ä»¥å°†åºåˆ—åˆ’åˆ†ä¸º L/a ä¸ªæ®µã€‚å¯¹äºå½“å‰çš„è¿™ä¸ªæ—¶å»¶æ®µï¼Œè®¡ç®—ä¸å…¶ä»–â€œæ®µâ€çš„ R å€¼ã€‚</p><p>æ—¶å»¶æ€»å…±æœ‰ k ç§ï¼ˆTop-kï¼‰ï¼Œa in {1, â€¦ , L}ã€‚a=1 æ—¶ï¼Œç­‰ä»·äºå½“å‰æ—¶é—´ç‚¹ä¸å…¶ä»–æ—¶é—´ç‚¹çš„ Rï¼Œè¦è®¡ç®— L æ¬¡ï¼›a=2 æ—¶ï¼Œå…±åˆ’åˆ†ä¸º L/2 ä¸ªâ€œæ®µâ€ï¼Œè¦è®¡ç®— L/2 æ¬¡â€¦â€¦[L+L/2+â€¦L/a]ï¼Œæ‰€ä»¥ O(k L)ï¼Œk=c lnLï¼ŒO(lnL L)ã€‚â€œæ›´æœ‰å¯èƒ½æ˜¯åºåˆ—å‘¨æœŸçš„ a å€¼å¯¹æ•´ä¸ªåºåˆ—è´¡çŒ®å¤§ï¼ˆåŠ æƒå¹³å‡ï¼‰â€</p><p>è€Œä¼ ç»Ÿçš„ self-attentionï¼Œæ˜¯ç‚¹å¯¹ç‚¹çš„ã€‚</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeurIPS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Informer-Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
      <link href="/paper%20notes/Informer/"/>
      <url>/paper%20notes/Informer/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #AAAI 2021<br>authors: Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang<br>code:</p></div><p><img src="/img/Informer.png" alt=""></p><p><img src="/img/Informer-1.png" alt=""></p><h2 id="ProbSparse-Self-attention">ProbSparse Self-attention</h2><p>å¯¹ transformer çš„ self-attention æœºåˆ¶è¿›è¡Œæ”¹è¿›ï¼Œç²¾ç®€ Qï¼ˆclogN)ã€‚(ç•¥æ‰äº†ä¸€äº› qiï¼Œä¸”åœ¨äº‹å…ˆè®¡ç®— M æ—¶å°±å¯¹ q åšäº†æŒ‘é€‰ï¼ŒLKlnLQï¼Œå†è®¡ç®—å‡º M åï¼Œå¯¹ top-u ä¸ªåš self-attention)<br>â€œTop-u queries under the sparsity measurement M (q, K), u=câ€</p><p>è‡ªæ³¨æ„åŠ›è’¸é¦ï¼Œæ”¹è¿›å†…å­˜ä½¿ç”¨ã€‚ï¼ˆå…ˆå·ç§¯åæœ€å¤§æ± åŒ–ï¼Œstride=2ï¼Œå¯¹ X(t) ä¸‹é‡‡æ ·ï¼Œä½¿ä¸‹ä¸€å±‚è¾“å…¥ä¸º L/2ï¼‰</p><p>ç”Ÿæˆå¼ï¼Œä¸€æ¬¡è¾“å‡ºæ‰€æœ‰é¢„æµ‹ç»“æœã€‚</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data</title>
      <link href="/paper%20notes/MSCRED/"/>
      <url>/paper%20notes/MSCRED/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>conference: #AAAI 2019<br>authors: Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu, Wei Cheng, Jingchao Ni, Bo Zong, Haifeng Chen, Nitesh V. Chawla<br>code: <a href="https://github.com/Zhang-Zhi-Jie/Pytorch-MSCRED">https://github.com/Zhang-Zhi-Jie/Pytorch-MSCRED</a></p></div><p><img src="/img/MSCRED-2.png" alt=""></p><p>1. <strong>Introduction and Conclusion</strong></p><p>Problemsï¼š<br>Cannot capture temporal dependencies across different time steps.Noise - affect the generalization capability, increase the false positive detections.The existing methods for root cause analysis are sensitive to noise and cannot handle this issue.</p><p>Contributionsï¼š</p><ol><li>anomaly detection and diagnosis problem as three underlying tasks, i.e., anomaly detection, root cause identification, and anomaly severity (duration) interpretation</li><li>MSCRED - the first model that considers correlations among multivariate time series for anomaly detection and can jointly resolve all the three tasks.</li></ol><p>ç»™å®š n ç»´æ—¶åºæ•°æ®ï¼Œå‡è®¾æ•°æ®ä¸å­˜åœ¨å¼‚å¸¸<br>Anomaly detectionï¼šæ£€æµ‹ t æ—¶åˆ»ä¹‹åæŸä¸€æ—¶åˆ»çš„å¼‚å¸¸äº‹ä»¶<br>Anomaly diagnosisï¼šè¯†åˆ«æœ€æœ‰å¯èƒ½å¯¼è‡´å¼‚å¸¸çš„æ—¶é—´åºåˆ—ï¼Œå¹¶ä¸”é‡åŒ–å¼‚å¸¸ç¨‹åº¦ã€‚<br>Intuitionï¼šå¦‚æœ MSCRED ä¹‹å‰ä»æœªè§‚å¯Ÿåˆ°ç±»ä¼¼çš„ç³»ç»ŸçŠ¶æ€ï¼Œå®ƒå¯èƒ½æ— æ³•å¾ˆå¥½åœ°é‡å»ºç­¾åçŸ©é˜µ<br>å…³äºä¸¥é‡ç¨‹åº¦ï¼Œæ–‡ä¸­å‡è®¾äº‹ä»¶çš„ä¸¥é‡æ€§ä¸å¼‚å¸¸çš„æŒç»­æ—¶é—´æˆæ­£æ¯”ã€‚</p><p><strong>MSCRED</strong></p><p>Multi-Scale Convolutional Recurrent Encoder-Decoder</p><p>multi-scale (resolution) signature matrices: characterize multiple levels of the system statuses</p><p>a convolutional encoder: encode the inter-sensor (time series) correlations</p><p>encode the inter-sensor (time series) correlations: capture the temporal patterns</p><p>convolutional decoder: reconstruct the input signature matrices</p><p>the residual signature matrices: detect and diagnose anomalies</p><p><img src="/img/MSCRED-3.png" alt=""></p><p><strong>Conclusion</strong></p><p>The framework is able to model both intersensor **correlations **and <strong>temporal dependencies</strong> of multivariate time series.</p><p>Extensive empirical studies on a synthetic dataset as well as a power plant dataset demonstrated that MSCRED can outperform <strong>stateof-the-art</strong> baseline methods.</p><p>2. <strong>Method</strong></p><p><img src="/img/MSCRED-2.png" alt=""></p><p><img src="/img/MSCRED.png" alt=""></p><p><img src="/img/MSCRED-4.png" alt=""></p><p><img src="/img/MSCRED-5.png" alt=""></p><p>æ•´ä¸ªè¿‡ç¨‹çš„æŸå¤±å‡½æ•°å®šä¹‰ä¸ºé‡æ„è¯¯å·®ã€‚åç»­åœ¨åŸå§‹ Signature Matrices å’Œé‡æ„ Signature Matrices çš„å·®å¼‚çŸ©é˜µä¸Šï¼Œè¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œè¯Šæ–­ã€‚</p><p>3. <strong>Experiments</strong></p><p><img src="/img/MSCRED-6.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
          <category> æ—¶é—´åºåˆ— </category>
          
      </categories>
      
      
        <tags>
            
            <tag> å¼‚å¸¸æ£€æµ‹ </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
